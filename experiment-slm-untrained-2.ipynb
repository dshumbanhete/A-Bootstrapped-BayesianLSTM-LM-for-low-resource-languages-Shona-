{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":461084,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":373169,"modelId":394016}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"NER.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1aTjGzC0bWjYVrB16D-EgdJTH7H6D0fYk\n\n#Adapted by Dean Shumbanhete\nMuch of the code used in this initial experiment was adapted from the code offered by Crash Course Computer Science, an online learning community. Original code can be found in the caption to the video found here:\nhttps://www.youtube.com/watch?v=oi0JXuL19TA\n\nPurpose of this is to try out some NLP models with Shona to see if dominant tools work well when transferring the same capabilities to Shona plain text. Original training and test data was taken from MasakhaneNER 2.0 (https://github.com/masakhane-io/masakhane-ner/tree/main/MasakhaNER2.0). This data format was already tagged and reduced to entity level entries and their associate tags which were used to train the model.\n\nAt first I had to flatten the data back into continuous text.\n\"\"\"\n\n# ---------- 0.  House-keeping ----------\n!pip -q install torchmetrics  # only extra dependency\nimport os, json, random, math, time, datetime\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.distributions.normal import Normal\nfrom torch.distributions.kl import kl_divergence\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n#--------------0. Globals-----------------------------\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# ---------- 1.  Device -----------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ---------- 2.  Data loading (from checkpoints)----------\ndef read_split(path: str) -> Tuple[List[List[str]], List[List[str]]]:\n    \"\"\"Return (sentences, ner_tags) lists.\"\"\"\n    lines = Path(path).read_text(encoding=\"utf-8\").splitlines()\n    sents, tags, cur_sent, cur_tags = [], [], [], []\n    for ln in lines:\n        ln = ln.strip()\n        if not ln:\n            if cur_sent:\n                sents.append(cur_sent); tags.append(cur_tags)\n                cur_sent, cur_tags = [], []\n        else:\n            tok, tag = ln.split()\n            cur_sent.append(tok); cur_tags.append(tag)\n    if cur_sent:                        # last example\n        sents.append(cur_sent); tags.append(cur_tags)\n    return sents, tags\n# Load the checkpoint\ndef run_from_saved():\n    try:\n        checkpoint_path = \"/kaggle/working/checkpoints/best.pt\"\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        \n        # Load the vocabulary and other metadata\n        meta_path = \"/kaggle/working/checkpoints/meta.json\"\n        with open(meta_path, \"r\") as f:\n            meta = json.load(f)\n        vocab_size = len(meta[\"itos\"])\n        vocab = meta[\"itos\"]\n        stoi = meta[\"stoi\"]\n        \n        # Create an instance of the model\n        model = BayesianLSTMMLM(vocab_size, cfg[\"embed_dim\"], cfg[\"hidden_dim\"]).to(device)\n        \n        # Load the state dictionary into the model\n        model.load_state_dict(checkpoint)\n        model.eval()  # Set the model to evaluation mode\n        \n        print(\"Model loaded successfully from checkpoint.\")\n        return True\n    except FileNotFoundError:\n        return False\n\n# ---------- 3.  Build vocabulary ----------\nclass Vocab:\n    def __init__(self, sentences: List[List[str]], min_freq: int):\n        counter = {}\n        for s in sentences:\n            for w in s:\n                counter[w] = counter.get(w, 0) + 1\n        self.itos = [\"<PAD>\", \"<UNK>\", \"<MASK>\"] + \\\n                    [w for w, c in counter.items() if c >= min_freq]\n        self.stoi = {w:i for i,w in enumerate(self.itos)}\n    def encode(self, sent): return [self.stoi.get(w, self.stoi[\"<UNK>\"]) for w in sent]\n\n\n# ---------- 4.  Configure some global variables into a dictionary for ease of use and readability----------\ncfg = dict(\n    data_root      = \"/kaggle/input/maskhaner/pytorch/default/4/\",   # change if local\n    vocab_min_freq = 2,\n    max_len        = 64,\n    batch_size     = 32,\n    embed_dim      = 128,\n    hidden_dim     = 256,\n    lr             = 1e-3,\n    mlm_prob       = 0.15,\n    epochs         = 150,\n    ensemble_masks = 5,\n    patience       = 5,\n    checkpoint_dir = \"/kaggle/working/checkpoints\",\n    seed           = SEED,\n)\nPath(cfg[\"checkpoint_dir\"]).mkdir(exist_ok=True, parents=True)\nif not (run_from_saved()):\n        train_loader = DataLoader(MLMDataset(train_sents),\n                                      batch_size=cfg[\"batch_size\"],\n                                      shuffle=True,\n                                      collate_fn=collate_mlm)\n        dev_loader   = DataLoader(MLMDataset(dev_sents),\n                                      batch_size=cfg[\"batch_size\"],\n                                      shuffle=False,\n                                      collate_fn=collate_mlm)\n        model = BayesianLSTMMLM(len(vocab.itos), cfg[\"embed_dim\"], cfg[\"hidden_dim\"]).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n        best_val, patience_left = 1e9, cfg[\"patience\"]\n        train_sents, train_tags = read_split(Path(cfg[\"data_root\"]) / \"train.txt\")\n        test_sents , test_tags  = read_split(Path(cfg[\"data_root\"]) / \"test.txt\")\n        dev_sents  , dev_tags   = read_split(Path(cfg[\"data_root\"]) / \"dev.txt\")\n        vocab = Vocab(train_sents, cfg[\"vocab_min_freq\"])\n        PAD, UNK, MASK = vocab.stoi[\"<PAD>\"], vocab.stoi[\"<UNK>\"], vocab.stoi[\"<MASK>\"]\n\n\n# ---------- 5.  Masked-LM ensemble wrapper ----------\nclass MLMDataset(Dataset):\n    def __init__(self, sentences):\n        self.data = [vocab.encode(s) for s in sentences]\n\n    def __len__(self): return len(self.data)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx][:cfg[\"max_len\"]]\n        padded = seq + [PAD]*(cfg[\"max_len\"] - len(seq))\n        return torch.tensor(padded, dtype=torch.long)\n# Create ensemble dataset\ndef collate_mlm(batch):\n    x_raw = torch.stack(batch)                       # [B, T]\n    B, T = x_raw.shape\n    K = cfg[\"ensemble_masks\"]\n\n    # one shared target\n    labels = x_raw.clone()\n    prob = torch.rand_like(labels, dtype=torch.float)\n    mask = prob < cfg[\"mlm_prob\"]\n    labels[~mask] = -100\n\n    # K different masked versions\n    x_ens = []\n    for _ in range(K):\n        x = x_raw.clone()\n        # 80% MASK, 10% random, 10% keep (per mask)\n        rand = prob < 0.8*cfg[\"mlm_prob\"]\n        x[rand] = MASK\n        rand = (prob >= 0.8*cfg[\"mlm_prob\"]) & (prob < 0.9*cfg[\"mlm_prob\"])\n        x[rand] = torch.randint_like(x[rand], low=3, high=len(vocab.itos))\n        # remaining 10% leave unchanged\n        x_ens.append(x)\n\n    x_ens = torch.stack(x_ens, dim=1)   # [B, K, T]\n    return x_ens.to(device), labels.to(device)\n\n\n\n# ---------- 6.  Bayesian Linear layer is a variational layer as in Blundell, C., Cornebise, J., Kavukcuoglu, K. and Wierstra, D., 2015, June. Weight uncertainty in neural network. In International conference on machine learning (pp. 1613-1622). PMLR.----------\n    \n    \nclass BayesianLinear(nn.Module):\n    def __init__(self, in_f, out_f, prior_sigma=1.0):\n        super().__init__()\n        self.in_f, self.out_f, self.prior_sigma = in_f, out_f, prior_sigma\n        # this is meant to simulate the rho(ρ) = log(1+exp(σ)), trick\n        self.w_mu  = nn.Parameter(torch.zeros(out_f, in_f))\n        self.w_rho = nn.Parameter(torch.ones(out_f, in_f)*-3)\n        self.b_mu  = nn.Parameter(torch.zeros(out_f))\n        self.b_rho = nn.Parameter(torch.ones(out_f)*-3)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        std = 1/math.sqrt(self.in_f)\n        nn.init.uniform_(self.w_mu, -std, std)\n        nn.init.uniform_(self.b_mu, -std, std)\n\n    def forward(self, x):\n        w_sigma = F.softplus(self.w_rho)\n        b_sigma = F.softplus(self.b_rho)\n        eps_w = torch.randn_like(self.w_mu)\n        eps_b = torch.randn_like(self.b_mu)\n        w = self.w_mu + w_sigma * eps_w\n        b = self.b_mu + b_sigma * eps_b\n        return F.linear(x, w, b)\n\n    def kl_loss(self):\n        prior = Normal(0, self.prior_sigma)\n        w_post = Normal(self.w_mu, F.softplus(self.w_rho))\n        b_post = Normal(self.b_mu, F.softplus(self.b_rho))\n        return kl_divergence(w_post, prior).sum() + kl_divergence(b_post, prior).sum() #bring in noise to help with the use of the prior to determin the posterior\n\n# ---------- 7.  Bayesian LSTM model implementation where much is borrowed from Zhu, L. and Laptev, N., 2017, November. Deep and confident prediction for time series at uber. In 2017 IEEE international conference on data mining workshops (ICDMW) (pp. 103-110). IEEE.----------\nclass BayesianLSTMMLM(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD)\n        self.lstm  = EnsembleLSTM(embed_dim, hidden_dim)\n        self.fc    = BayesianLinear(hidden_dim, vocab_size)\n\n    def forward(self, x):                # x : [B, K, T]\n        emb = self.embed(x)              # [B, K, T, E]\n        out = self.lstm(emb)             # [B, T, H]\n        return self.fc(out)              # [B, T, V]\n\n    def kl_loss(self):\n        return self.fc.kl_loss()\n   \n    # LSTM that receives [B, K, T, E] and returns [B, T, H]\n    # by concatenating outputs along the hidden dim and then\n    # projecting back to hidden_dim.\n    \nclass EnsembleLSTM(nn.Module):\n    def __init__(self, embed_dim, hidden_dim):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        # projection from K*hidden_dim back to hidden_dim\n        self.merge = nn.Linear(hidden_dim * cfg[\"ensemble_masks\"], hidden_dim)\n\n    def forward(self, x_emb):\n        # x_emb : [B, K, T, E]\n        B, K, T, E = x_emb.shape\n        x_emb = x_emb.view(B*K, T, E)          # treat as larger batch\n        out, _ = self.lstm(x_emb)              # [B*K, T, H]\n        out = out.view(B, K, T, -1)            # [B, K, T, H]\n        out = out.permute(0, 2, 1, 3).contiguous()  # [B, T, K, H]\n        out = out.view(B, T, -1)               # [B, T, K*H]\n        return torch.tanh(self.merge(out))     # [B, T, H]\n\n# ---------- 8.  Training ----------\n    \ndef run_epoch(loader, training=False):\n    total_loss, total_acc, n_tok = 0., 0., 0\n    if training: model.train()\n    else:        model.eval()\n    torch.set_grad_enabled(training)\n    for x, y in loader:\n        if training: optimizer.zero_grad()\n        logits = model(x)\n        loss_ce = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-100)\n        if training:\n            loss_kl = model.kl_loss() / len(train_loader.dataset)  # scale KL\n            loss = loss_ce + loss_kl\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        else:\n            loss = loss_ce\n        total_loss += loss_ce.item() * y.numel()\n        preds = logits.argmax(-1)\n        mask = y != -100\n        total_acc += (preds[mask] == y[mask]).sum().item()\n        n_tok  += mask.sum().item()\n    return total_loss/n_tok, total_acc/n_tok  # avg CE, accuracy\n    \n#--------------9. Evaluate and generate sample generations----------------------------------\ndef generate(prompt_tokens, max_new=30, temperature=0.5):\n    ids = vocab.encode(prompt_tokens)\n    K = cfg[\"ensemble_masks\"]\n\n    for _ in range(max_new):\n        # build 4-D tensor [1, K, T]\n        x = torch.tensor(ids[-cfg[\"max_len\"]:], dtype=torch.long, device=device)\n        x = x.unsqueeze(0).expand(K, -1).unsqueeze(0)  # [1, K, T]\n\n        with torch.no_grad():\n            logits = model(x)              # [1, T, V]\n            # average over the ensemble dimension (K) after softmax for stability\n            probs = F.softmax(logits / temperature, dim=-1)  # [1, T, V]\n            ens_prob = probs.mean(1)                           # [1, V]\n            tok = torch.multinomial(ens_prob, 1).item()\n\n        ids.append(tok)\n\n    return \" \".join(vocab.itos[i] for i in ids)\n\n\ndef main():        \n    print(\"Vocab size:\", len(vocab.itos))\n   \n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"])\n    best_val, patience_left = 1e9, cfg[\"patience\"]\n    for epoch in range(1, cfg[\"epochs\"]+1):\n        train_loss, train_acc = run_epoch(train_loader, training=True)\n        val_loss, val_acc     = run_epoch(dev_loader, training=False)\n        print(f\"Ep {epoch:02d}  \"\n              f\"train_ppl={math.exp(train_loss):6.2f}  \"\n              f\"train_acc={train_acc*100:5.1f}%  |  \"\n              f\"val_ppl={math.exp(val_loss):6.2f}  \"\n              f\"val_acc={val_acc*100:5.1f}%\")\n        # Early stopping\n        if val_loss < best_val:\n            best_val = val_loss\n            torch.save(model.state_dict(), Path(cfg[\"checkpoint_dir\"])/\"best.pt\")\n            patience_left = cfg[\"patience\"]\n        else:\n            patience_left -= 1\n            if patience_left == 0:\n                print(\"Early stopping.\")\n                break\n    \n    # ---------- 10.  Generation demo ----------\n    model.load_state_dict(torch.load(Path(cfg[\"checkpoint_dir\"])/\"best.pt\"))\n    model.eval()\n        \n    print(f\"Train: {len(train_sents)}  Dev: {len(dev_sents)}  Test: {len(test_sents)}\")\n    example = generate([\"Moro\", \"Vakuru\"], max_new=25)\n    print(\"Generation:\", example)\n    \n    \n    # ---------- 11.  Save artefacts ----------\n    meta = {\n        \"cfg\": cfg,\n        \"itos\": vocab.itos,\n        \"stoi\": vocab.stoi,\n    }\n    json.dump(meta, open(Path(cfg[\"checkpoint_dir\"])/\"meta.json\", \"w\"))\n    print(\"Saved artefacts to\", cfg[\"checkpoint_dir\"])\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T11:09:14.956255Z","iopub.execute_input":"2025-08-11T11:09:14.956591Z","iopub.status.idle":"2025-08-11T11:09:56.318248Z","shell.execute_reply.started":"2025-08-11T11:09:14.956554Z","shell.execute_reply":"2025-08-11T11:09:56.317190Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nModel loaded successfully from checkpoint.\nVocab size: 9616\nEp 01  train_ppl=1603590.61  train_acc= 71.7%  |  val_ppl=4502089.63  val_acc= 72.2%\nEp 02  train_ppl=6929381.04  train_acc= 70.9%  |  val_ppl=4236470.76  val_acc= 71.8%\nEp 03  train_ppl=7595246.39  train_acc= 70.9%  |  val_ppl=9247161.54  val_acc= 71.5%\nEp 04  train_ppl=7320650.78  train_acc= 71.2%  |  val_ppl=6382112.91  val_acc= 72.2%\nEp 05  train_ppl=8533019.58  train_acc= 70.9%  |  val_ppl=15890548.99  val_acc= 71.2%\nEp 06  train_ppl=9199290.80  train_acc= 71.1%  |  val_ppl=8998381.31  val_acc= 72.3%\nEp 07  train_ppl=13163552.51  train_acc= 70.7%  |  val_ppl=7422958.54  val_acc= 72.9%\nEarly stopping.\nTrain: 6207  Dev: 887  Test: 1773\nGeneration: <UNK> Vakuru <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> vanosvika <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\nSaved artefacts to /kaggle/working/checkpoints\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}