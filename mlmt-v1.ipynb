{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":460996,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":373169,"modelId":394016}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nWritten By Dean Shumbanhete. Much of the code was taken verbatim from the seminal implementation \"The Annotated Transformer\" by Harvard NLP\nThis is a seminal resource which implements the original \"Attention Is All You Need\" paper.\nThis model will be used as the baseline to compare the effect of bootstrap techniques on NLP processing.\n\nEncoder maps an input sequence of symbol representations (x1,…,xn) to a sequence of continuous representations z=(z1,…,zn).\nGiven z, the decoder then generates an output sequence (y1,…,ym) of symbols one element at a time. At each step the model is auto-regressive,\nconsuming the previously generated symbols as additional input when generating the next.\n\"\"\"\n!pip install accelerate -q\n\nimport numpy as np\nimport math\nimport pandas as pd\nimport seaborn as sns\nsns.set_context(context=\"talk\")\nfrom torch.utils.data import DataLoader, Dataset \nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport copy\nfrom collections import Counter, defaultdict\nimport json \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport time\nfrom tqdm import tqdm\nimport random\nimport os\nfrom accelerate import Accelerator\n\n\n\n#----------------------1. Housekeeping------------------------------------\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nglobal max_src_in_batch, max_tgt_in_batch\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \nprint(\"Device:\", device) \n\ncfg = dict(\n    data_root      = \"/kaggle/input/maskhaner/pytorch/default/1/\",   # change if needed\n    vocab_min_freq = 0,\n    max_len        = 64,\n    batch_size     = 32,\n    embed_dim      = 128,\n    hidden_dim     = 256,\n    lr             = 1e-3,\n    mlm_prob       = 0.15,\n    epochs         = 100,\n    ensemble_masks = 5,\n    patience       = 5,\n    checkpoint_dir = \"/kaggle/working/checkpoints\",\n    seed           = SEED,\n)\n\n#-----------------------2. Architecture------------------------------------------\nclass EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        #At each forward pass the model processes the masked source and target sequences\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n\n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n# --------‐ Vocabulary & Data Reading --------\n\nclass Generator(nn.Module):\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n    def forward(self, x):\n        return F.log_softmax(self.proj(x), dim=-1)\n\ndef read_split(path: str):\n    \"\"\"Return list of token lists from file (tokenized).\"\"\"\n    lines = Path(path).read_text(encoding=\"utf-8\").splitlines()\n    sents = []\n    for ln in lines:\n        ln = ln.strip()\n        if not ln:\n            continue\n        # assume tokenized: tokens separated by spaces\n        toks = ln.split()\n        sents.append(toks)\n    return sents\n\nclass Vocab:\n    def __init__(self, sentences, min_freq: int):\n        counter = {}\n        for s in sentences:\n            for w in s:\n                counter[w] = counter.get(w, 0) + 1\n        self.itos = [\"<PAD>\", \"<MASK>\", \"<UNK>\"] + [w for w, c in counter.items() if c >= min_freq]\n        self.stoi = {w:i for i,w in enumerate(self.itos)}\n        self.pad_index = self.stoi[\"<PAD>\"]\n        self.mask_index = self.stoi[\"<MASK>\"]\n        self.unk_index = self.stoi[\"<UNK>\"]\n\n    def encode(self, sent):\n        return [ self.stoi.get(w, self.unk_index) for w in sent]\n\n# --------‐ Dataset and Collate fn for MLM with Ensemble Masks --------\n\nclass MLMDataset(Dataset):\n    def __init__(self, vocab, sentences):\n        self.vocab = vocab\n        self.sentences = sentences\n        #Fix for problems with vocabulary being referenced before being assigned\n\n    def __len__(self):\n        return len(self.sentences)\n\n\n    #if the sentence is too short for seq len then add the pad token/index to the remaining slots in the sequence. Needed to work with tensors.\n    def __getitem__(self, idx):\n        seq = self.vocab.encode(self.sentences[idx])[:cfg[\"max_len\"]]\n        PAD = self.vocab.pad_index\n        if len(seq) < cfg[\"max_len\"]:\n            seq = seq + [PAD] * (cfg[\"max_len\"] - len(seq))\n        return torch.tensor(seq, dtype=torch.long)\n\ndef collate_mlm(batch, vocab):\n    \"\"\"\n    batch: list of sequences [seq_len] each\n    returns:\n      x_ens: [B, K, T] masked inputs (ensemble versions),\n      labels: [B, T] with -100 on non‐masked positions,\n      src_mask: [B, 1, T] mask for padding tokens\n      Note: copied from Bayesian. Could encapsulate these in classes for better readability and reusability.\n    \"\"\"\n    x_raw = torch.stack(batch)                       # [B, T]\n    B, T = x_raw.shape\n    K = cfg[\"ensemble_masks\"]\n\n    PAD, UNK, MASK = vocab.stoi[\"<PAD>\"], vocab.stoi[\"<UNK>\"], vocab.stoi[\"<MASK>\"]\n\n    # one shared target (labels)\n    labels = x_raw.clone()\n    prob = torch.rand_like(labels, dtype=torch.float)\n    mask = prob < cfg[\"mlm_prob\"]\n    labels[~mask] = -100  # positions not masked are ignored in loss\n\n    # K different masked versions\n    x_ens = []\n    for _ in range(K):\n        x = x_raw.clone()\n        # 80% of mask positions => MASK token\n        m1 = (prob < 0.8 * cfg[\"mlm_prob\"])\n        x[m1] = MASK\n        # 10% => random token\n        m2 = (prob >= 0.8 * cfg[\"mlm_prob\"]) & (prob < 0.9 * cfg[\"mlm_prob\"])\n        # sample random tokens (ignore PAD, UNK etc.)\n        rand_tokens = torch.randint(low=3, high=len(vocab.itos), size=(B, T), device=x.device)\n        x[m2] = rand_tokens[m2]\n        # 10% remain unchanged\n        x_ens.append(x)\n\n    x_ens = torch.stack(x_ens, dim=1)   # [B, K, T]\n    src_mask = (x_ens != PAD).unsqueeze(1)  # [B, 1, T]\n    return x_ens.to(device), labels.to(device), src_mask.to(device)\n\n# --------‐ Model Definition --------\n\ndef clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\nclass Encoder(nn.Module):\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n\n    def forward(self, x, mask):\n      #This is where we do the m,asking of the input through each layer. We simulate this in the Bayesian network masked ensemble.\n      #In this model we leverage the multihead attention mechanism as well as parallelism. In the Bayesian network we use the dataloader to reduce\n      #data dependencies, allowing us to work in a quasi-concurrent manner.\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n\nclass LayerNorm(nn.Module):\n  #Defines a residual connection around each of the two sub layers, followed by layer normalisation.\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n      #Output of each sublayer is LayerNorm ( x+ Sublayer (x)) where Sublayer(x) is the function implemented by the sub-layer itself.\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\nclass SublayerConnection(nn.Module):\n  # A residual layer followed by layer normalisation.\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))\n\nclass EncoderLayer(nn.Module):\n  #Class used to define the sublayers for each layer. By applying dropout to the output of each sublayers\n  #we produce output predictions of dimension 512. These sublayers contain the multi-head self attention m,echanism,\n  # and the position-wise fully connected neural network (feed-forawrd).\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n\n    def forward(self, x, mask):\n      #TO DO\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        x = self.sublayer[1](x, self.feed_forward)\n        return x\n\nclass Decoder(nn.Module):\n  #Class defines a decoder stack composed of N=6 identical layers. Decoder inserts a third sub-layer, which performs mutli-head attention ovver the output of the encoder stack. \n  #We therefore need to apply residual connectiosn around the sub-layers, followed by layer normalisation. \n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)\n\n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        x = self.sublayer[2](x, self.feed_forward)\n        return x\n    def subsequent_mask(size):\n      attn_shape = (1, size, size)\n      subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n      return torch.from_numpy(subsequent_mask)==0\n\n\n\"\"\"\nAttention\n\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n\nThe authors call their particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension dk,\nand values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values. Apart from Dot-product attention, there is also additive attention which compurtes the compatability function using a feed-forward network with a single hidden layer.\n\not-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk.\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Where the projections are parameter matrices WQi∈Rdmodel×dk, WKi∈Rdmodel×dk, WVi∈Rdmodel×dv and WO∈Rhdv×dmodel.\nIn this model the authors employ h=8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h=64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n\nAdditional to the attention sublayers, each of the layers contains a fully connected feedforward network which is appplied to each position seperately and identically.\nAnnalogous to two convolutions with kernel size 1. The dimensionality of input and output is dmodel=512, and the inner-layer has dimensionality dff=2048.\n\n\"\"\"\n\ndef attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e4)\n    p_attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super(MultiHeadedAttention, self).__init__()\n        self.h = h\n        self.d_k = d_model // h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        seq_len = query.size(1)\n        #Do linear projections in each batch => h x d_k\n        query, key, value = [lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2) for lin, x in zip(self.linears[:-1], (query, key, value))]\n        #Apply the attention on all the projection vectors for this batch\n        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n        \n        #Concatenate using the view function, and apply a linear filter over output.\n        x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\n        return self.linears[-1](x)\n\nclass PositionWiseFeedForward(nn.Module):\n  #Class defines a model that uses learned embeddings to convert the input tokens and output tokens to vectors of dimension d=512\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionWiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n\nclass Embeddings(nn.Module):\n  #Class defines the learned linear transformation and softmax function to convert the decoder output into predicted token probabilities.\n    def __init__(self, vocab_size, d_model):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab_size, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-(math.log(10000.0) / d_model)))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)        \n        return self.dropout(x)\n\n\ndef make_m(src_vocab_size, tgt_vocab_size, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model, dropout)\n    ff = PositionWiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    model = EncoderDecoder(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        nn.Sequential(Embeddings(src_vocab_size, d_model), c(position)),\n        nn.Sequential(Embeddings(tgt_vocab_size, d_model), c(position)),\n        Generator(d_model, tgt_vocab_size)\n    )\n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    return model\n\n  \nclass NoamOpt:\n  def __init__(self, model_size, factor, warmup, optimizer):\n    self.optimizer = optimizer\n    self._step = 0\n    self.warmup = warmup\n    self.factor = factor\n    self.model_size = model_size\n    self._rate = 0\n\n  def rate(self):\n    return self.factor * (self.model_size ** (-0.5) * min(self._step ** (-0.5), self._step * self.warmup ** (-1.5)))\n\n  def step(self):\n    self._step += 1\n    rate = self.rate()\n    for p in self.optimizer.param_groups:\n      p['lr'] = rate\n    self._rate = rate\n    self.optimizer.step()\n\n# --------‐ Loss & LossCompute modifications for MLM ---------------------------------------------------------------------\n\nclass LabelSmoothing(nn.Module):\n    \"Implement label smoothing (if you want; optional).\"\n    def __init__(self, size, padding_idx, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(size_average=False)\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n\n    def forward(self, x, target):\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.criterion(x, Variable(true_dist, requires_grad=False))\n\nclass LossCompute:\n    def __init__(self, generator, opt=None):\n        self.generator = generator\n        self.opt = opt\n\n    def __call__(self, out, labels):\n        \"\"\"\n        out: [B*K, T, V] (logits from model before generator if generator is separate, else adjust)\n        labels: [B*K, T] with -100 for non‐mask positions\n        returns: loss (averaged over masked tokens)\n        \"\"\"\n      \n        # here assume out is the raw transformer outputs (before generator)\n        logits = self.generator(out)  # [B*K, T, Vocab]\n\n        Bk, T, V = logits.size()\n        logits_flat = logits.contiguous().view(-1, V)           # [Bk * T, V]\n        labels_flat = labels.contiguous().view(-1)              # [Bk * T]\n        loss = F.cross_entropy(logits_flat, labels_flat, ignore_index=-100)\n\n        if self.opt is not None:\n            loss.backward()\n            self.opt.optimizer.zero_grad()\n            self.opt.step()\n\n        # Return total loss, plus count of masked tokens if prefered; here return loss and masked count\n        # Compute number of non‐ignore positions\n        masked = (labels_flat != -100).sum().item()\n        return loss.item(), masked\n\n# --------‐ Training / Evaluation Loop ---------------------------------------------------------------------\n\ndef run_mlm(data_loader, model, loss_compute, vocab, training=True):\n    \n    total_loss = 0.0\n    total_masked = 0\n    if training:\n        model.train()\n       \n        data_iter = tqdm(data_loader, desc=\"Training\", leave=False)\n    else:\n        model.eval()\n        data_iter = tqdm(data_loader, desc=\"Validation\", leave=False)\n    torch.set_grad_enabled(training)\n    for batch in data_iter:\n            x_ens, labels, src_mask = batch   # x_ens: [B, K]\n            B, K, T = x_ens.size()\n           \n            x_input = x_ens.view(B * K, T)  # [B*K, T]\n            labels = labels.to(device) \n            src_mask_input = (x_input == vocab.pad_index).unsqueeze(1)  # [B*K, 1, T]\n            out = model.forward(x_input, x_input, src_mask_input, src_mask_input)  # shape [B*K, T, d_model]\n            \n            loss_val, masked_count = loss_compute(out, labels.repeat_interleave(K, dim=0))\n\n            total_loss += loss_val * masked_count\n            total_masked += masked_count\n\n    avg_loss = total_loss / total_masked if total_masked > 0 else float(\"inf\")\n    perplexity = math.exp(avg_loss) if avg_loss < float(\"inf\") else float(\"inf\")\n    return avg_loss, perplexity\n\n# --------‐---------------- Main--------------------------------------------------------------------\n\ndef main():\n    # 1. Initialize the Accelerator\n    accelerator = Accelerator(mixed_precision=\"fp16\")\n    device = accelerator.device # Use accelerator's device\n\n    train_sents = read_split(Path(cfg[\"data_root\"]) / \"train.txt\")\n    dev_sents   = read_split(Path(cfg[\"data_root\"]) / \"dev.txt\")\n\n    vocab = Vocab(train_sents, cfg[\"vocab_min_freq\"])\n    V = len(vocab.itos)\n    print(\"Vocab size:\", V)\n\n    train_loader = DataLoader(MLMDataset(vocab, train_sents),\n                              batch_size=cfg[\"batch_size\"],\n                              shuffle=True,\n                              collate_fn=lambda b: collate_mlm(b, vocab))\n    dev_loader   = DataLoader(MLMDataset(vocab, dev_sents),\n                              batch_size=cfg[\"batch_size\"],\n                              shuffle=False,\n                              collate_fn=lambda b: collate_mlm(b, vocab))\n\n    model = make_m(V, V, N=2, d_model=cfg[\"embed_dim\"], d_ff=cfg[\"hidden_dim\"], dropout=0.1) \n    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n\n    # 3. Wrap model, optimizer, and data loaders with accelerator.prepare()\n    model, model_opt, train_loader, dev_loader = accelerator.prepare(\n        model, model_opt, train_loader, dev_loader\n    )\n\n    loss_compute = LossCompute(model.generator, opt=model_opt)\n\n    best_val_loss = float(\"inf\")\n    print(\"Running Training...\")\n    for epoch in range(1, cfg[\"epochs\"] + 1):\n        start = time.time()\n        \n        # Pass device from accelerator to run_mlm\n        train_avg_loss, train_perplexity = run_mlm(train_loader, model, loss_compute, vocab, training=True)\n        val_avg_loss, val_perplexity = run_mlm(dev_loader, model, LossCompute(model.generator, opt=None), vocab, training=False)\n\n\n        elapsed = time.time() - start\n        print(f\"Epoch {epoch:02d} | train_loss_per_masked={math.exp(train_avg_loss):6.2f} | train_perplexity={train_perplexity*100:5.1f} | val_loss_per_masked={math.exp(val_avg_loss):6.2f} | val_perplexity={val_perplexity*100:5.1f} | time={elapsed:.1f}s\")\n\n\n        if val_avg_loss < best_val_loss:\n            best_val_loss = val_avg_loss\n            os.makedirs(cfg[\"checkpoint_dir\"], exist_ok=True)\n            \n            accelerator.save_state(Path(cfg[\"checkpoint_dir\"]) / f\"best_mlm_epoch{epoch}\")\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:18:28.238658Z","iopub.execute_input":"2025-09-15T17:18:28.238999Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nVocab size: 24937\nRunning Training...\n","output_type":"stream"},{"name":"stderr","text":"Training:  12%|█▏        | 521/4289 [01:43<12:42,  4.94it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}